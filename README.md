# [ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation](https://manicm-fast.github.io/)

[Project Page](https://manicm-fast.github.io/) | [arXiv](http://arxiv.org/abs/2406.01586) | [Paper](https://arxiv.org/pdf/2406.01586)

[Guanxing Lu](https://guanxinglu.github.io/)\*, Zifeng Gao\*, [Tianxing Chen](https://tianxingchen.github.io), [Wenxun Dai](https://github.com/Dai-Wenxun), [Ziwei Wang](https://ziweiwangthu.github.io/), [Yansong Tang](https://andytang15.github.io/)<sup>â€ </sup>

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FManiCM-fast%2FManiCM&count_bg=%2349CCFF&title_bg=%23B038EF&icon=&icon_color=%23E7E7E7&title=ManiCM+Code+Viewers&edge_flat=false)](https://hits.seeyoufarm.com)

![](./files/2024-ManiCM.png)
<b>ManiCM Overview</b>: Given a raw action sequence a<sub>0</sub>, we first perform a forward diffusion to introduce noise over n + k steps. The resulting noisy sequence a<sub>n+k</sub> is then fed into both the online network and the teacher network to predict the clean action sequence. The target network uses the teacher networkâ€™s k-step estimation results to predict the action sequence. To enforce self-consistency, a loss function is applied to ensure that the outputs of the online network and the target network are consistent.

# ğŸ’» Installation

See [INSTALL.md](INSTALL.md) for installation instructions. 



# ğŸ“š Config

**Algorithms**. We provide the implementation of the following algorithms: 

- DP3: `dp3.yaml`
- ManiCM: `dp3_cm.yaml`

You can modify the configuration of the teacher model and ManiCM by editing these two files. Here are the meanings of some important configurations:

`num_inference_timesteps`: The inference steps of ManiCM.

`num_train_timesteps`: Total time step for adding noise.

`prediction_type`:  `epsilon` represents prediction noise, while `sample` represents predicted action.

For more detailed arguments, please refer to the scripts and the code.

# ğŸ› ï¸ Usage

Scripts for generating demonstrations, training, and evaluation are all provided in the `scripts/` folder. 

The results are logged by `wandb`, so you need to `wandb login` first to see the results and videos.

We provide a simple instruction for using the codebase here.

1. Generate demonstrations by `gen_demonstration_adroit.sh` and `gen_demonstration_dexart.sh`. See the scripts for details. For example:
    ```bash
    bash scripts/gen_demonstration_adroit.sh hammer
    ```
    This will generate demonstrations for the `hammer` task in Adroit environment. The data will be saved in `3D-Diffusion-Policy/data/` folder automatically.


2. Train and evaluate a teacher policy with behavior cloning. For example:
    ```bash
    # bash scripts/train_policy.sh config_name task_name addition_info seed gpu_id 
    bash scripts/train_policy.sh dp3 adroit_hammer 0603 0 0
    ```
    This will train a DP3 policy on the `hammer` task in Adroit environment using point cloud modality. By default we **save** the ckpt (optional in the script). During training, teacher's model takes ~10G gpu memory and ~4 hours on an Nvidia 4090 GPU.
    
3. Move teacher's ckpt. For example:
    ```bash
    # bash scopy.sh alg_name task_name teacher_addition_info addition_info seed gpu_id
    bash scopy.sh dp3_cm adroit_hammer 0603 0603_cm 0 0
    ```
    
4. Train and evaluate ManiCM. For example:
    ```bash
    # bash scripts/train_policy_cm.sh config_name task_name addition_info seed gpu_id
    bash scripts/train_policy_cm.sh dp3_cm adroit_hammer 0603_cm 0 0
    ```
    This will train ManiCM use a DP3 policy teacher model on the `hammer` task in Adroit environment using point cloud modality. During training, ManiCM model takes ~10G gpu memory and ~4 hours on an Nvidia 4090 GPU.

# ğŸï¸ Checkpoints

We have updated the [pre-trained checkpoints](https://drive.google.com/drive/folders/1WhYQij_D3IisDpdLjKCQF3iaZB1bZ63f?usp=sharing) of `hammer` task in Adroit environment for your convenience. You can download them and place the folder into `data/outputs/`.

# ğŸ·ï¸ License
This repository is released under the MIT license.

# ğŸ™ Acknowledgement

Our code is built upon [3D Diffusion Policy](https://github.com/YanjieZe/3D-Diffusion-Policy), [MotionLCM](https://github.com/Dai-Wenxun/MotionLCM), [Latent Consistency Model](https://github.com/luosiallen/latent-consistency-model), [Diffusion Policy](https://github.com/real-stanford/diffusion_policy), [VRL3](https://github.com/microsoft/VRL3), [Metaworld](https://github.com/Farama-Foundation/Metaworld), and [ManiGaussian](https://github.com/GuanxingLu/ManiGaussian). We would like to thank the authors for their excellent works.

# ğŸ¥° Citation
If you find this repository helpful, please consider citing:

```
@article{lu2024manicm,
      title={ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation}, 
      author={Guanxing Lu and Zifeng Gao and Tianxing Chen and Wenxun Dai and Ziwei Wang and Yansong Tang},
      journal={arXiv preprint arXiv:2406.01586},
      year={2024}
}
```

# 31 Tasks (Adroit and MetaWorld)
<div id="video-container"></div>
<script>
    // è·å–è§†é¢‘å®¹å™¨ï¼ˆç¡®ä¿IDä¸HTMLä¸­çš„divæ ‡ç­¾ä¸€è‡´ï¼‰
    var container = document.getElementById('video-container');

    // å‡½æ•°ç”¨äºåˆ›å»ºè§†é¢‘å…ƒç´ å¹¶æ·»åŠ åˆ°å®¹å™¨
    function createVideoElement(index) {
        var video = document.createElement('video');
        video.className = 'video-item'; // ç¡®ä¿è¿™ä¸ªç±»åä¸CSSä¸­çš„ç±»åä¸€è‡´
        video.setAttribute('controls', '');
        video.setAttribute('autoplay', '');
        video.setAttribute('loop', '');
        video.muted = true; // é™éŸ³è‡ªåŠ¨æ’­æ”¾
    
        var source = document.createElement('source');
        source.src = 'videos/' + index + '.mp4';
        source.type = 'video/mp4';
    
        video.appendChild(source);
    
        // è§†é¢‘åŠ è½½å®Œæˆåè‡ªåŠ¨æ’­æ”¾
        video.addEventListener('loadedmetadata', function() {
            video.style.display = 'block'; // æ˜¾ç¤ºè§†é¢‘
            video.play(); // æ’­æ”¾è§†é¢‘
        });
    
        return video;
    }
    
    // åŠ¨æ€åˆ›å»ºå¹¶æ·»åŠ 32ä¸ªè§†é¢‘åˆ°é¡µé¢
    for (var i = 1; i <= 32; i++) {
        container.appendChild(createVideoElement(i));
    }
</script>

<style>
  #video-container { /* ä½¿ç”¨IDé€‰æ‹©å™¨æ¥åŒ¹é…ä¸Šé¢çš„divæ ‡ç­¾ */
    display: grid;
    grid-template-columns: repeat(8, 1fr); /* 8åˆ— */
    grid-template-rows: repeat(4, auto); /* 4è¡Œï¼Œé«˜åº¦æ ¹æ®å†…å®¹è‡ªåŠ¨è°ƒæ•´ */
    gap: 10px; /* ç½‘æ ¼é¡¹ä¹‹é—´çš„é—´éš™ */
    width: 100%; /* å®¹å™¨å®½åº¦ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´ */
    margin: auto; /* å±…ä¸­æ˜¾ç¤º */
    padding: 5px; /* å®¹å™¨å†…è¾¹è· */
    border: 2px dashed #333; /* è™šçº¿è¾¹æ¡† */
    box-sizing: border-box; /* è¾¹æ¡†è®¡ç®—åœ¨å®½åº¦å†… */
  }
  .video-item {
    width: 100%; /* è§†é¢‘å®½åº¦ */
    height: auto; /* è§†é¢‘é«˜åº¦è‡ªé€‚åº” */
    display: block; /* æ˜¾ç¤ºè§†é¢‘ */
  }
</style>